{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1bd2141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\hirom\\\\NLP\\\\LLM\\\\fcc-gpt-course',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\python39.zip',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\DLLs',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3',\n",
      " '',\n",
      " 'C:\\\\Users\\\\hirom\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
      " 'D:\\\\NLP\\\\JParaCrawl',\n",
      " 'C:\\\\users\\\\hirom\\\\nlp\\\\llm\\\\cuda\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "pprint.pprint(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11199a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('D:\\\\NLP\\\\JParaCrawl')\n",
    "sys.path.append('C:\\\\users\\\\hirom\\\\nlp\\\\llm\\cuda\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d19e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\hirom\\\\NLP\\\\LLM\\\\fcc-gpt-course',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\python39.zip',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\DLLs',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3',\n",
      " '',\n",
      " 'C:\\\\Users\\\\hirom\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
      " 'C:\\\\Users\\\\hirom\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
      " 'D:\\\\NLP\\\\JParaCrawl',\n",
      " 'C:\\\\users\\\\hirom\\\\nlp\\\\llm\\\\cuda\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a774cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a7c920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0bfbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "en_train_path = 'D:\\\\NLP\\\\JParaCrawl\\\\JP_en_train.txt'\n",
    "en_test_path = 'D:\\\\NLP\\\\JParaCrawl\\\\JP_en_test.txt'\n",
    "ja_train_path = 'D:\\\\NLP\\\\JParaCrawl\\\\JP_ja_train.txt'\n",
    "ja_test_path = 'D:\\\\NLP\\\\JParaCrawl\\\\JP_ja_test.txt'\n",
    "\n",
    "en_train = load_data(en_train_path)\n",
    "en_test = load_data(en_test_path)\n",
    "ja_train = load_data(ja_train_path)\n",
    "ja_test = load_data(ja_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83849a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\hirom\\nlp\\llm\\cuda\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 714.29 MB. The target location C:\\Users\\hirom\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\blobs only has 547.57 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3828ee9b825e4b7ca8906232893e4bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  70%|#######   | 503M/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\hirom\\nlp\\llm\\cuda\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hirom\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "bert_masked_model = BertForMaskedLM.from_pretrained(model_name, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e9ab9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing texts:  23%|███████████▌                                      | 1831469/7887595 [17:24<1:00:01, 1681.69it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "processing texts: 100%|██████████████████████████████████████████████████| 7887595/7887595 [1:13:42<00:00, 1783.34it/s]\n",
      "processing texts: 100%|██████████████████████████████████████████████████████| 876400/876400 [08:24<00:00, 1735.80it/s]\n",
      "processing texts: 100%|██████████████████████████████████████████████████| 7887595/7887595 [1:03:55<00:00, 2056.62it/s]\n",
      "processing texts: 100%|██████████████████████████████████████████████████████| 876400/876400 [08:53<00:00, 1641.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal max_length: 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_max_length(text_list, tokenizer):\n",
    "    max_length = 0\n",
    "    for text in tqdm(text_list, desc=\"processing texts\"):\n",
    "        encoded_text = tokenizer(text, add_special_tokens=True, padding=False, truncation=False)\n",
    "        text_length = len(encoded_text['input_ids'])\n",
    "        max_length = max(max_length, text_length)\n",
    "    return max_length\n",
    "\n",
    "en_train_max_length = get_max_length(en_train, tokenizer)\n",
    "en_test_max_length = get_max_length(en_test, tokenizer)\n",
    "ja_train_max_length = get_max_length(ja_train, tokenizer)\n",
    "ja_test_max_length = get_max_length(ja_test, tokenizer)\n",
    "\n",
    "max_length = max(en_train_max_length, en_test_max_length, ja_train_max_length, ja_test_max_length)\n",
    "print(f\"Optimal max_length: {max_length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
