{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "19Pbbdy7j7Y8BnyOwEimnXAKvXfpiJ-fK",
      "authorship_tag": "ABX9TyM9WBia8092vonz0PnhNhvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiromi06/machine-translation/blob/main/MarianMT_train_chunk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vFrby8QbDJiK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import MarianMTModel, MarianTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ymxwsolLDMG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea618eb5-b6cc-475b-d348-42d1e57c9c4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_data, label_data):\n",
        "        self.input_ids = input_data['input_ids']\n",
        "        self.attention_mask = input_data['attention_mask']\n",
        "        self.labels = label_data['input_ids']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def load_chunk(file_path):\n",
        "    return torch.load(file_path)\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def train_on_chunk(model, optimizer, scheduler, device, input_chunk_path, label_chunk_path, batch_size=16):\n",
        "    input_data = load_chunk(input_chunk_path)\n",
        "    label_data = load_chunk(label_chunk_path)\n",
        "\n",
        "    #print(f\"Input data input_ids shape: {input_data['input_ids'].shape}\")\n",
        "    #print(f\"Label data input_ids shape: {label_data['input_ids'].shape}\")\n",
        "\n",
        "\n",
        "    dataset = TranslationDataset(input_data, label_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    loop = tqdm(dataloader, leave=True)\n",
        "    for batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "         # トークンIDの範囲をチェック\n",
        "        #print(f\"Max token ID in input_ids: {input_ids.max()}\")\n",
        "        #print(f\"Min token ID in input_ids: {input_ids.min()}\")\n",
        "        #print(f\"Max token ID in labels: {labels.max()}\")\n",
        "        #print(f\"Min token ID in labels: {labels.min()}\")\n",
        "\n",
        "        # 追加：データの範囲チェック\n",
        "        #print(f\"Batch input_ids shape: {input_ids.shape}\")\n",
        "        #print(f\"Batch attention_mask shape: {attention_mask.shape}\")\n",
        "        #print(f\"Batch labels shape: {labels.shape}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            # 入力形状を確認\n",
        "            #print(f\"Input IDs: {input_ids.shape}, Attention Mask: {attention_mask.shape}, Labels: {labels.shape}\")\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            # 出力の形状を確認\n",
        "            #print(f\"Model outputs shape: {outputs.logits.shape}\")\n",
        "            #print(f\"Model loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loop.set_postfix(loss=total_loss/len(dataloader))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(dataloader)\n",
        "    print(f\"Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "def validate_on_chunks(model, device, input_chunk_paths, label_chunk_paths, batch_size=16):\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for input_chunk_path, label_chunk_path in zip(input_chunk_paths, label_chunk_paths):\n",
        "        input_data = load_chunk(input_chunk_path)\n",
        "        label_data = load_chunk(label_chunk_path)\n",
        "\n",
        "        dataset = TranslationDataset(input_data, label_data)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loop = tqdm(dataloader, leave=True)\n",
        "            for i, batch in enumerate(loop):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                loop.set_postfix(loss=total_eval_loss / ((i+1) * len(dataloader)))\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(input_chunk_paths)\n",
        "    print(f\"Validation Loss: {avg_val_loss}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "B5mV6G5HDOdp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/machine_learning/MarianMT/ep3/marian_model_chunk4_ep3'\n",
        "original_model_name = 'Helsinki-NLP/opus-mt-ja-en'\n",
        "\n",
        "model = MarianMTModel.from_pretrained(model_dir)\n",
        "#print(\"model: \", model)\n",
        "tokenizer = MarianTokenizer.from_pretrained(original_model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "encoded_data_dir = '/content/drive/MyDrive/machine_learning/MarianMT/Marian_encoded_data'\n",
        "ja_train_chunk = os.path.join(encoded_data_dir, 'ja_train_encoded_chunk_5.pt')\n",
        "en_train_chunk = os.path.join(encoded_data_dir, 'en_train_encoded_chunk_5.pt')\n",
        "ja_test_chunk = os.path.join(encoded_data_dir, 'ja_test_encoded_chunk_5.pt')\n",
        "en_test_chunk = os.path.join(encoded_data_dir, 'en_test_encoded_chunk_5.pt')\n",
        "\n",
        "\n",
        "num_training_steps = 1 * 1\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "print(\"Training on the chunk\")\n",
        "train_on_chunk(model, optimizer, scheduler, device, ja_train_chunk, en_train_chunk, batch_size=16)\n",
        "\n",
        "# Perform validation on the first chunk\n",
        "print(\"Validation on the chunk\")\n",
        "validate_on_chunks(model, device, [ja_test_chunk], [en_test_chunk], batch_size=16)\n",
        "\n",
        "# Save the model\n",
        "model_save_path_2 = '/content/drive/MyDrive/machine_learning/MarianMT/ep3/marian_model_chunk5_ep3'\n",
        "model.save_pretrained(model_save_path_2)\n",
        "tokenizer.save_pretrained(model_save_path_2)\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "def format_time(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    seconds = seconds % 60\n",
        "    return f\"Processing time: {hours}h {minutes}m {seconds:.2f}s\"\n",
        "\n",
        "print(format_time(processing_time))"
      ],
      "metadata": {
        "id": "PclF7qUbDZYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cb09cc-8e2a-4f93-c44a-9227ce1ad08c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "<ipython-input-7-03961ebf5778>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file_path)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on the chunk\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-03961ebf5778>:37: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "  0%|          | 0/49298 [00:00<?, ?it/s]<ipython-input-7-03961ebf5778>:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "100%|██████████| 49298/49298 [1:36:55<00:00,  8.48it/s, loss=0.59]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5900007378295002\n",
            "Validation on the chunk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5478/5478 [10:02<00:00,  9.09it/s, loss=0.000104]\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[60715]], 'forced_eos_token_id': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3127.723754912615\n",
            "Processing time: 1h 48m 48.62s\n"
          ]
        }
      ]
    }
  ]
}